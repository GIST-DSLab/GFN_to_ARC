#!/usr/bin/env python3
"""
ì „ì²´ LLM ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„° ì „ì²˜ë¦¬ â†’ í•™ìŠµ â†’ ì¶”ë¡  ë° í‰ê°€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
"""

import os
import sys
import time
import subprocess
import argparse
from utils import *
import logging

def run_command(command: str, description: str, logger, capture_output: bool = False):
    """ëª…ë ¹ì–´ ì‹¤í–‰ ë° ë¡œê¹…"""
    logger.info(f"{'='*50}")
    logger.info(f"Starting: {description}")
    logger.info(f"Command: {command}")
    logger.info(f"{'='*50}")
    
    start_time = time.time()
    
    try:
        if capture_output:
            # ì¶œë ¥ì„ ìº¡ì²˜í•˜ëŠ” ê²½ìš° (ë°ì´í„° ì „ì²˜ë¦¬, ì¶”ë¡  ë“±)
            result = subprocess.run(
                command, 
                shell=True, 
                check=True, 
                capture_output=True, 
                text=True
            )
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… Completed: {description} ({elapsed_time:.2f}s)")
            
            if result.stdout:
                logger.info(f"STDOUT:\\n{result.stdout}")
            if result.stderr:
                logger.warning(f"STDERR:\\n{result.stderr}")
        else:
            # ì‹¤ì‹œê°„ ì¶œë ¥ì´ í•„ìš”í•œ ê²½ìš° (í•™ìŠµ ë“±)
            logger.info(f"ğŸš€ Running command with live output...")
            result = subprocess.run(
                command, 
                shell=True, 
                check=True
            )
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… Completed: {description} ({elapsed_time:.2f}s)")
            
        return True
        
    except subprocess.CalledProcessError as e:
        elapsed_time = time.time() - start_time
        logger.error(f"âŒ Failed: {description} ({elapsed_time:.2f}s)")
        logger.error(f"Exit code: {e.returncode}")
        
        if capture_output:
            logger.error(f"STDOUT:\\n{e.stdout}")
            logger.error(f"STDERR:\\n{e.stderr}")
        
        return False

def check_prerequisites(config: Dict, logger):
    """ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    logger.info("Checking prerequisites...")
    
    # trajectory ë°ì´í„° í™•ì¸
    trajectory_dir = config['trajectory_data_dir']
    if not os.path.exists(trajectory_dir):
        logger.error(f"Trajectory data directory not found: {trajectory_dir}")
        return False
    
    # í•„ìš”í•œ trajectory íŒŒì¼ë“¤ í™•ì¸
    missing_files = []
    for problem_id in config['problem_mapping'].keys():
        trajectory_file = os.path.join(
            trajectory_dir, 
            f"problem_{problem_id}", 
            "trajectories_0_1000.json"
        )
        if not os.path.exists(trajectory_file):
            missing_files.append(trajectory_file)
    
    if missing_files:
        logger.warning(f"Missing trajectory files: {missing_files}")
        logger.warning("Some problems will be skipped during processing")
    
    # ReARC ë°ì´í„° í™•ì¸
    rearc_dir = config['rearc_data_dir']
    if not os.path.exists(rearc_dir):
        logger.error(f"ReARC data directory not found: {rearc_dir}")
        return False
    
    # í•„ìš”í•œ Python íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = [
        'torch', 'transformers', 'numpy', 'yaml', 
        'sklearn', 'wandb', 'matplotlib'
    ]
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            logger.error(f"Required package not found: {package}")
            return False
    
    logger.info("âœ… All prerequisites checked")
    return True

def setup_directories(config: Dict, logger):
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    directories = [
        config['processed_data_dir'],
        config['model_save_dir'],
        config["results_dir"],
        "./logs"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        logger.info(f"Created/verified directory: {directory}")

def run_preprocessing(config: Dict, logger, force: bool = False, config_path: str = "configs/config.yaml"):
    """ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰"""
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    processed_file = os.path.join(config['processed_data_dir'], "all_training_data.json")
    if os.path.exists(processed_file) and not force:
        logger.info("Preprocessed data already exists. Skipping preprocessing.")
        logger.info("Use --force-preprocessing to regenerate.")
        return True
    
    # all_training_data.jsonì´ ì—†ì–´ë„ ê°œë³„ problem íŒŒì¼ë“¤ì´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
    import glob
    problem_files = glob.glob(os.path.join(config['processed_data_dir'], "problem_*_processed.json"))
    if problem_files and not force:
        # ë¹ˆ íŒŒì¼ì´ ì•„ë‹Œ ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        valid_files = []
        for problem_file in problem_files:
            try:
                with open(problem_file, 'r') as f:
                    data = f.read().strip()
                    if data and data != "[]" and len(data) > 10:  # ë¹ˆ ë°°ì—´ì´ ì•„ë‹ˆê³  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                        valid_files.append(problem_file)
            except Exception:
                continue
        
        if valid_files:
            logger.info(f"Found {len(valid_files)} existing processed data files. Skipping preprocessing.")
            logger.info(f"Files: {[os.path.basename(f) for f in valid_files]}")
            logger.info("Use --force-preprocessing to regenerate.")
            return True
    
    python_path = "/home/ubuntu/miniconda3/envs/gflownet/bin/python"
    command = f"{python_path} data_preprocessing.py --config {config_path}"
    return run_command(command, "Data Preprocessing", logger, capture_output=False)  # tqdm í‘œì‹œë¥¼ ìœ„í•´ Falseë¡œ ë³€ê²½

def run_training(config: Dict, logger, gpu_ids: str = None, force: bool = False, config_path: str = "configs/config.yaml", use_unsloth: bool = False):
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
    
    # Unsloth ì‚¬ìš© ì‹œ ë‹¤ë¥¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸
    if use_unsloth:
        model_dir = os.path.join(config['model_save_dir'], "unsloth_lora_model")
    else:
        model_dir = os.path.join(config['model_save_dir'], f"arc_action_model_{config['model_name'].split('/')[-1]}", "final_model")
    
    if os.path.exists(model_dir) and not force:
        logger.info("Trained model already exists. Skipping training.")
        logger.info("Use --force-training to retrain.")
        return True
    
    # Unsloth ì‚¬ìš© ì‹œ ë‹¨ì¼ GPU ì „ìš©
    if use_unsloth:
        logger.info("Using Unsloth for accelerated training (single GPU only)")
        python_path = "/home/ubuntu/miniconda3/envs/gflownet/bin/python"  # gflow-llm í™˜ê²½ ì‚¬ìš©
        command = f"{python_path} training_unsloth.py --config {config_path}"
        return run_command(command, "Unsloth Training", logger, capture_output=False)
    
    # GPU ê°œìˆ˜ì— ë”°ë¼ DDP ë˜ëŠ” ì¼ë°˜ python ì‚¬ìš©
    if gpu_ids:
        gpu_list = gpu_ids.split()
        num_gpus = len(gpu_list)
        
        if num_gpus > 1:
            # ë©€í‹° GPU: torchrun ì‚¬ìš©
            logger.info(f"Using DDP with torchrun on {num_gpus} GPUs: {gpu_ids}")
            os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(gpu_list)
            
            # torchrunì„ í†µí•œ DDP í•™ìŠµ (gflow-llm í™˜ê²½ ì‚¬ìš©)
            python_path = "/home/ubuntu/miniconda3/envs/gflownet/bin/python"
            command = f"{python_path} -m torch.distributed.run --nproc-per-node={num_gpus} --nnodes=1 --standalone training.py --config {config_path}"
            return run_command(command, "Model Training", logger, capture_output=False)
        else:
            # ë‹¨ì¼ GPU: ì¼ë°˜ python ì‚¬ìš© (gflow-llm í™˜ê²½)
            logger.info(f"Using single GPU: {gpu_ids}")
            python_path = "/home/ubuntu/miniconda3/envs/gflownet/bin/python"
            command = f"{python_path} training.py --gpu_ids {gpu_ids} --config {config_path}"
            return run_command(command, "Model Training", logger, capture_output=False)
    else:
        # GPU ì§€ì • ì—†ìŒ: ì¼ë°˜ python ì‚¬ìš© (gflow-llm í™˜ê²½)
        python_path = "/home/ubuntu/miniconda3/envs/gflownet/bin/python"
        command = f"{python_path} training.py --config {config_path}"
        return run_command(command, "Model Training", logger, capture_output=False)

def run_inference(config: Dict, logger, gpu_ids: str = None, force: bool = False, config_path: str = "configs/config.yaml"):
    """ì¶”ë¡  ë° í‰ê°€ ì‹¤í–‰"""
    
    # ì´ë¯¸ í‰ê°€ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
    results_file = os.path.join(config['results_dir'], "evaluation_results.json")
    if os.path.exists(results_file) and not force:
        logger.info("Evaluation results already exist. Skipping inference.")
        logger.info("Use --force-inference to re-evaluate.")
        return True
    
    python_path = "/home/ubuntu/miniconda3/envs/gflownet/bin/python"
    command = f"{python_path} inference.py --config {config_path}"
    if gpu_ids:
        command += f" --gpu_ids {gpu_ids}"
    return run_command(command, "Inference and Evaluation", logger)

def generate_summary_report(config: Dict, logger):
    """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    try:
        results_file = os.path.join(config['results_dir'], "evaluation_results.json")
        if not os.path.exists(results_file):
            logger.warning("No evaluation results found for summary report")
            return
        
        results = load_json(results_file)
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        summary = {
            "experiment_summary": {
                "model_name": config['model_name'],
                "problems_trained": list(config['problem_mapping'].keys()),
                "overall_accuracy": results.get('overall_stats', {}).get('overall_accuracy', 0),
                "total_tests": results.get('overall_stats', {}).get('total_tests', 0),
                "total_correct": results.get('overall_stats', {}).get('total_correct', 0)
            },
            "problem_breakdown": {}
        }
        
        # ë¬¸ì œë³„ ì„¸ë¶€ ê²°ê³¼
        for arc_id, problem_result in results.get('problem_results', {}).items():
            summary["problem_breakdown"][arc_id] = {
                "accuracy": problem_result.get('accuracy', 0),
                "correct_count": problem_result.get('correct_count', 0),
                "total_count": problem_result.get('total_count', 0)
            }
        
        # ìš”ì•½ ë³´ê³ ì„œ ì €ì¥
        summary_file = os.path.join(config['results_dir'], "experiment_summary.json")
        save_json(summary, summary_file)
        
        # ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥
        logger.info("\\n" + "="*60)
        logger.info("EXPERIMENT SUMMARY REPORT")
        logger.info("="*60)
        logger.info(f"Model: {config['model_name']}")
        logger.info(f"Problems Trained: {len(config['problem_mapping'])}")
        logger.info(f"Overall Accuracy: {summary['experiment_summary']['overall_accuracy']:.3f}")
        logger.info(f"Total Tests: {summary['experiment_summary']['total_tests']}")
        logger.info(f"Total Correct: {summary['experiment_summary']['total_correct']}")
        logger.info("\\nProblem Breakdown:")
        
        for arc_id, stats in summary["problem_breakdown"].items():
            logger.info(f"  {arc_id}: {stats['accuracy']:.3f} ({stats['correct_count']}/{stats['total_count']})")
        
        logger.info("="*60)
        logger.info(f"Summary saved to: {summary_file}")
        
    except Exception as e:
        logger.error(f"Error generating summary report: {e}")

def main():
    parser = argparse.ArgumentParser(description="Run complete LLM experiment for ARC action sequence learning")
    parser.add_argument("--config", default="configs/config.yaml", help="Configuration file path")
    parser.add_argument("--gpu_ids", nargs='+', type=int, default=None, help="Specific GPU IDs to use (e.g., --gpu_ids 6 7)")
    parser.add_argument("--skip-preprocessing", action="store_true", help="Skip data preprocessing step")
    parser.add_argument("--skip-training", action="store_true", help="Skip model training step")
    parser.add_argument("--skip-inference", action="store_true", help="Skip inference and evaluation step")
    parser.add_argument("--force-preprocessing", action="store_true", help="Force rerun preprocessing even if data exists")
    parser.add_argument("--force-training", action="store_true", help="Force retrain model even if exists")
    parser.add_argument("--force-inference", action="store_true", help="Force re-evaluate even if results exist")
    parser.add_argument("--preprocessing-only", action="store_true", help="Run only preprocessing step")
    parser.add_argument("--training-only", action="store_true", help="Run only training step")
    parser.add_argument("--inference-only", action="store_true", help="Run only inference step")
    parser.add_argument("--use-unsloth", action="store_true", help="Use Unsloth for accelerated training (single GPU only)")
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    
    # ë¡œê¹… ì„¤ì •
    log_dir = config.get('environment', {}).get('log_dir', './logs')
    log_file = os.path.join(log_dir, "experiment.log")
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    logger = setup_logging(log_file)
    
    logger.info("ğŸš€ Starting LLM Experiment for ARC Action Sequence Learning")
    logger.info(f"Configuration: {args.config}")
    logger.info(f"Model: {config['model_name']}")
    
    start_time = time.time()
    
    try:
        # ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not check_prerequisites(config, logger):
            logger.error("Prerequisites check failed. Exiting.")
            return 1
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        setup_directories(config, logger)
        
        # ë‹¨ê³„ë³„ ì‹¤í–‰
        success = True
        
        # ì „ì²˜ë¦¬ ë‹¨ê³„
        if not args.skip_preprocessing and (not args.training_only and not args.inference_only):
            if not run_preprocessing(config, logger, args.force_preprocessing, args.config):
                success = False
                logger.error("Preprocessing failed")
                
        if args.preprocessing_only:
            logger.info("Preprocessing completed. Exiting as requested.")
            return 0
        
        # GPU IDs ë¬¸ìì—´ ë³€í™˜
        gpu_ids_str = " ".join(map(str, args.gpu_ids)) if args.gpu_ids else None
        
        # í•™ìŠµ ë‹¨ê³„
        if success and not args.skip_training and (not args.preprocessing_only and not args.inference_only):
            if not run_training(config, logger, gpu_ids_str, args.force_training, args.config, args.use_unsloth):
                success = False
                logger.error("Training failed")
                
        if args.training_only:
            logger.info("Training completed. Exiting as requested.")
            return 0
        
        # ì¶”ë¡  ë° í‰ê°€ ë‹¨ê³„
        if success and not args.skip_inference and not args.preprocessing_only:
            if not run_inference(config, logger, gpu_ids_str, args.force_inference, args.config):
                success = False
                logger.error("Inference failed")
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        if success:
            generate_summary_report(config, logger)
        
        # ìµœì¢… ê²°ê³¼
        total_time = time.time() - start_time
        
        if success:
            logger.info(f"ğŸ‰ Experiment completed successfully! Total time: {total_time:.2f}s ({total_time/60:.1f}m)")
            return 0
        else:
            logger.error(f"âŒ Experiment failed. Total time: {total_time:.2f}s")
            return 1
            
    except KeyboardInterrupt:
        logger.info("Experiment interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)