# LLM Experiment Configuration - 20250706 version with ARC-style few-shot learning

# Data paths
trajectory_data_dir: "./data/trajectories_output"
rearc_data_dir: "./data/re-arc"
processed_data_dir: "./processed_data"
model_save_dir: "./models"
results_dir: "./results"

# Training parameters
model_name: "microsoft/DialoGPT-medium"
max_length: 2048  # Increased for few-shot examples
batch_size: 4  # Smaller batch for longer sequences
learning_rate: 0.00001  # Lower learning rate for fine-tuning
num_epochs: 5  # Fewer epochs for initial test
warmup_steps: 100
gradient_accumulation_steps: 4

# Data preprocessing
max_grid_size: 30
action_mapping:
  0: "left_rotate" # 왼쪽 회전
  1: "right_rotate" # 오른쪽 회전
  2: "horizontal_flip" # 수평 뒤집기
  3: "vertical_flip" # 수직 뒤집기
  4: "submit" # 제출

# Available problem IDs and their ARC task IDs (only available trajectory files)
problem_mapping:
  86: "25ff71a9"
  139: "6150a2bd"
  178: "74dd1130"
  149: "67a3c6ac"  # Fixed mapping
  154: "68b16354"  # Fixed mapping  
  240: "9dfd6313"
  379: "ed36ccf7"  # Fixed mapping

# Evaluation settings
evaluation:
  few_shot_examples: 3
  test_examples_range: [3, 8]  # Use examples 3-8 for testing
  
# Generation settings
generation:
  inference_temperature: 0.1
  do_sample: true
  early_stopping: true

num_test_samples: 50
max_action_sequence_length: 20

# Environment Configuration
environment:
  python_paths:
    gflownet: "/data/miniforge3/bin/python"
    gflow_llm: "/data/miniforge3/bin/python"
  sys_paths:
    - "/home/ubuntu/GFN_to_ARC/gfn/src"
    - "/home/ubuntu/GFN_to_ARC/gfn/src/ARCenv"
  log_dir: "./logs"