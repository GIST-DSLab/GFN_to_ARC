# LLM Experiment Configuration - GPU 7 (Problem 379)

# Data paths
trajectory_data_dir: "/opt/dlami/nvme/seungpil"
rearc_data_dir: "./data/re-arc"
processed_data_dir: "./processed_data_gpu7"
model_save_dir: "/opt/dlami/nvme/seungpil/models_gpu7"
results_dir: "/opt/dlami/nvme/seungpil/results_gpu7"

# Training parameters
model_name: "meta-llama/Llama-3.1-8B-Instruct"
max_length: 1024
batch_size: 1
learning_rate: 0.00005
num_epochs: 1
warmup_steps: 50
gradient_accumulation_steps: 16

# Data preprocessing
max_grid_size: 30
action_mapping:
  0: "left_rotate"
  1: "right_rotate"
  2: "horizontal_flip"
  3: "vertical_flip"
  4: "submit"

# Available problem IDs (GPU 7: 379)
problem_mapping:
  379: "ecdecbb3"

# Evaluation
num_test_samples: 50
max_action_sequence_length: 20
