# LLM Experiment Configuration - GPU 4 (Problem 86, 139)

# Data paths
trajectory_data_dir: "/opt/dlami/nvme/seungpil"
rearc_data_dir: "./data/re-arc"
processed_data_dir: "./processed_data_gpu4"
model_save_dir: "/opt/dlami/nvme/seungpil/models_gpu4"
results_dir: "/opt/dlami/nvme/seungpil/results_gpu4"

# Training parameters
model_name: "meta-llama/Llama-3.1-8B-Instruct"
max_length: 1024
batch_size: 1
learning_rate: 0.00005
num_epochs: 1
warmup_steps: 50
gradient_accumulation_steps: 16

# Data preprocessing
max_grid_size: 30
action_mapping:
  0: "left_rotate"
  1: "right_rotate"
  2: "horizontal_flip"
  3: "vertical_flip"
  4: "submit"

# Available problem IDs (GPU 4: 86, 139)
problem_mapping:
  86: "25ff71a9"
  139: "6150a2bd"

# Evaluation
num_test_samples: 50
max_action_sequence_length: 20
