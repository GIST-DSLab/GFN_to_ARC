#!/usr/bin/env python3
"""
Evaluation ì •í™•ë„ ê³„ì‚° ë¡œì§ì˜ ì •í™•ì„± í…ŒìŠ¤íŠ¸
"""

import os
import json
import numpy as np
from typing import List, Dict, Any


class EvaluationAccuracyTester:
    """Evaluation ì •í™•ë„ ê³„ì‚° ë¡œì§ í…ŒìŠ¤íŠ¸"""
    
    def test_single_problem_accuracy(self):
        """ë‹¨ì¼ ë¬¸ì œ ì •í™•ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        print("=== ë‹¨ì¼ ë¬¸ì œ ì •í™•ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸ ===")
        
        test_cases = [
            {
                'name': 'ëª¨ë“  í…ŒìŠ¤íŠ¸ ì •ë‹µ',
                'results': [
                    {'is_correct': True},
                    {'is_correct': True},
                    {'is_correct': True}
                ],
                'expected_accuracy': 1.0,
                'expected_correct': 3,
                'expected_total': 3
            },
            {
                'name': 'ëª¨ë“  í…ŒìŠ¤íŠ¸ ì˜¤ë‹µ',
                'results': [
                    {'is_correct': False},
                    {'is_correct': False},
                    {'is_correct': False}
                ],
                'expected_accuracy': 0.0,
                'expected_correct': 0,
                'expected_total': 3
            },
            {
                'name': 'í˜¼í•© ê²°ê³¼',
                'results': [
                    {'is_correct': True},
                    {'is_correct': False},
                    {'is_correct': True},
                    {'is_correct': False}
                ],
                'expected_accuracy': 0.5,
                'expected_correct': 2,
                'expected_total': 4
            },
            {
                'name': 'ì˜¤ë¥˜ê°€ í¬í•¨ëœ ê²°ê³¼',
                'results': [
                    {'is_correct': True},
                    {'error': 'Some error', 'is_correct': False},
                    {'is_correct': True},
                    {'error': 'Another error', 'is_correct': False}
                ],
                'expected_accuracy': 0.5,
                'expected_correct': 2,
                'expected_total': 4
            },
            {
                'name': 'ë¹ˆ ê²°ê³¼',
                'results': [],
                'expected_accuracy': 0.0,
                'expected_correct': 0,
                'expected_total': 0
            }
        ]
        
        all_passed = True
        
        for test_case in test_cases:
            print(f"\ní…ŒìŠ¤íŠ¸: {test_case['name']}")
            results = test_case['results']
            
            # inference.pyì˜ ë¡œì§ ì¬í˜„
            correct_count = sum(1 for r in results if r.get('is_correct', False))
            total_count = len(results)
            accuracy = correct_count / total_count if total_count > 0 else 0.0
            
            # ê²€ì¦
            expected_correct = test_case['expected_correct']
            expected_total = test_case['expected_total']
            expected_accuracy = test_case['expected_accuracy']
            
            correct_match = correct_count == expected_correct
            total_match = total_count == expected_total
            accuracy_match = abs(accuracy - expected_accuracy) < 1e-6
            
            passed = correct_match and total_match and accuracy_match
            all_passed = all_passed and passed
            
            print(f"  ê²°ê³¼ ìˆ˜: {total_count} (ì˜ˆìƒ: {expected_total}) {'âœ“' if total_match else 'âœ—'}")
            print(f"  ì •ë‹µ ìˆ˜: {correct_count} (ì˜ˆìƒ: {expected_correct}) {'âœ“' if correct_match else 'âœ—'}")
            print(f"  ì •í™•ë„: {accuracy:.4f} (ì˜ˆìƒ: {expected_accuracy:.4f}) {'âœ“' if accuracy_match else 'âœ—'}")
            print(f"  ì „ì²´ í…ŒìŠ¤íŠ¸: {'PASS' if passed else 'FAIL'}")
        
        return all_passed
    
    def test_overall_accuracy_calculation(self):
        """ì „ì²´ ì •í™•ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        print("\n=== ì „ì²´ ì •í™•ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸ ===")
        
        # ì—¬ëŸ¬ ë¬¸ì œì˜ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
        problem_results = [
            {
                'problem_id': 86,
                'correct_count': 2,
                'total_count': 3,
                'accuracy': 2/3
            },
            {
                'problem_id': 139,
                'correct_count': 1,
                'total_count': 2,
                'accuracy': 0.5
            },
            {
                'problem_id': 178,
                'correct_count': 3,
                'total_count': 3,
                'accuracy': 1.0
            }
        ]
        
        # inference.pyì˜ ë¡œì§ ì¬í˜„
        total_correct = sum(result['correct_count'] for result in problem_results)
        total_tests = sum(result['total_count'] for result in problem_results)
        overall_accuracy = total_correct / total_tests if total_tests > 0 else 0.0
        
        # ìˆ˜ë™ ê³„ì‚°
        expected_total_correct = 2 + 1 + 3  # 6
        expected_total_tests = 3 + 2 + 3    # 8
        expected_overall_accuracy = 6 / 8   # 0.75
        
        print(f"ë¬¸ì œë³„ ê²°ê³¼:")
        for result in problem_results:
            print(f"  ë¬¸ì œ {result['problem_id']}: {result['correct_count']}/{result['total_count']} = {result['accuracy']:.3f}")
        
        print(f"\nì „ì²´ í†µê³„:")
        print(f"  ì´ ì •ë‹µ ìˆ˜: {total_correct} (ì˜ˆìƒ: {expected_total_correct})")
        print(f"  ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {total_tests} (ì˜ˆìƒ: {expected_total_tests})")
        print(f"  ì „ì²´ ì •í™•ë„: {overall_accuracy:.4f} (ì˜ˆìƒ: {expected_overall_accuracy:.4f})")
        
        # ê²€ì¦
        correct_match = total_correct == expected_total_correct
        total_match = total_tests == expected_total_tests
        accuracy_match = abs(overall_accuracy - expected_overall_accuracy) < 1e-6
        
        passed = correct_match and total_match and accuracy_match
        
        print(f"ê²€ì¦ ê²°ê³¼: {'PASS' if passed else 'FAIL'}")
        
        return passed
    
    def test_edge_cases(self):
        """ê²½ê³„ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        print("\n=== ê²½ê³„ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ===")
        
        edge_cases = [
            {
                'name': 'ëª¨ë“  ë¬¸ì œê°€ ë¹ˆ ê²°ê³¼',
                'problem_results': [
                    {'correct_count': 0, 'total_count': 0},
                    {'correct_count': 0, 'total_count': 0}
                ],
                'expected_overall': 0.0
            },
            {
                'name': 'ì¼ë¶€ ë¬¸ì œë§Œ ë¹ˆ ê²°ê³¼',
                'problem_results': [
                    {'correct_count': 2, 'total_count': 2},
                    {'correct_count': 0, 'total_count': 0},
                    {'correct_count': 1, 'total_count': 2}
                ],
                'expected_overall': 3/4  # (2+0+1)/(2+0+2)
            },
            {
                'name': 'ë‹¨ì¼ ë¬¸ì œ ì™„ë²½ ì •ë‹µ',
                'problem_results': [
                    {'correct_count': 5, 'total_count': 5}
                ],
                'expected_overall': 1.0
            },
            {
                'name': 'ë‹¨ì¼ ë¬¸ì œ ì™„ì „ ì˜¤ë‹µ',
                'problem_results': [
                    {'correct_count': 0, 'total_count': 5}
                ],
                'expected_overall': 0.0
            }
        ]
        
        all_passed = True
        
        for case in edge_cases:
            print(f"\ní…ŒìŠ¤íŠ¸: {case['name']}")
            problem_results = case['problem_results']
            expected = case['expected_overall']
            
            # ê³„ì‚°
            total_correct = sum(r['correct_count'] for r in problem_results)
            total_tests = sum(r['total_count'] for r in problem_results)
            overall_accuracy = total_correct / total_tests if total_tests > 0 else 0.0
            
            # ê²€ì¦
            passed = abs(overall_accuracy - expected) < 1e-6
            all_passed = all_passed and passed
            
            print(f"  ë¬¸ì œ ê²°ê³¼: {problem_results}")
            print(f"  ê³„ì‚°ëœ ì •í™•ë„: {overall_accuracy:.4f}")
            print(f"  ì˜ˆìƒ ì •í™•ë„: {expected:.4f}")
            print(f"  í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'PASS' if passed else 'FAIL'}")
        
        return all_passed
    
    def test_floating_point_precision(self):
        """ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„ í…ŒìŠ¤íŠ¸"""
        print("\n=== ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„ í…ŒìŠ¤íŠ¸ ===")
        
        # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¼€ì´ìŠ¤ë“¤
        precision_cases = [
            {
                'name': '1/3 ì •í™•ë„',
                'correct': 1,
                'total': 3,
                'expected': 1/3
            },
            {
                'name': '2/3 ì •í™•ë„',
                'correct': 2,
                'total': 3,
                'expected': 2/3
            },
            {
                'name': '1/7 ì •í™•ë„',
                'correct': 1,
                'total': 7,
                'expected': 1/7
            },
            {
                'name': 'í° ìˆ˜ì˜ ì •í™•ë„',
                'correct': 999999,
                'total': 1000000,
                'expected': 999999/1000000
            }
        ]
        
        all_passed = True
        
        for case in precision_cases:
            print(f"\ní…ŒìŠ¤íŠ¸: {case['name']}")
            correct = case['correct']
            total = case['total']
            expected = case['expected']
            
            # ê³„ì‚°
            accuracy = correct / total if total > 0 else 0.0
            
            # ê²€ì¦ (ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ í—ˆìš©)
            passed = abs(accuracy - expected) < 1e-10
            all_passed = all_passed and passed
            
            print(f"  ì •ë‹µ/ì´ìˆ˜: {correct}/{total}")
            print(f"  ê³„ì‚°ëœ ì •í™•ë„: {accuracy}")
            print(f"  ì˜ˆìƒ ì •í™•ë„: {expected}")
            print(f"  ì°¨ì´: {abs(accuracy - expected)}")
            print(f"  í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'PASS' if passed else 'FAIL'}")
        
        return all_passed
    
    def test_result_data_structure(self):
        """ê²°ê³¼ ë°ì´í„° êµ¬ì¡° í…ŒìŠ¤íŠ¸"""
        print("\n=== ê²°ê³¼ ë°ì´í„° êµ¬ì¡° í…ŒìŠ¤íŠ¸ ===")
        
        # inference.pyì—ì„œ ìƒì„±ë˜ëŠ” ë°ì´í„° êµ¬ì¡° ì‹œë®¬ë ˆì´ì…˜
        evaluation_summary = {
            'overall_accuracy': 0.75,
            'total_correct': 6,
            'total_tests': 8,
            'problem_results': [
                {
                    'problem_id': 86,
                    'test_results': [
                        {
                            'test_idx': 0,
                            'predicted_actions': [0, 4],
                            'predicted_grid': [[3, 6, 9], [2, 5, 8], [1, 4, 7]],
                            'expected_grid': [[3, 6, 9], [2, 5, 8], [1, 4, 7]],
                            'is_correct': True,
                            'confidence': 0.85,
                            'num_actions': 2
                        },
                        {
                            'test_idx': 1,
                            'predicted_actions': [2, 4],
                            'predicted_grid': [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                            'expected_grid': [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
                            'is_correct': True,
                            'confidence': 0.72,
                            'num_actions': 2
                        }
                    ],
                    'accuracy': 1.0,
                    'correct_count': 2,
                    'total_count': 2
                }
            ],
            'model_config': {'n_layer': 3, 'n_head': 4},
            'evaluation_problems': [86, 139, 178]
        }
        
        # ë°ì´í„° êµ¬ì¡° ê²€ì¦
        required_top_level = ['overall_accuracy', 'total_correct', 'total_tests', 'problem_results']
        required_problem_level = ['problem_id', 'test_results', 'accuracy', 'correct_count', 'total_count']
        required_test_level = ['test_idx', 'is_correct']
        
        # ìƒìœ„ ë ˆë²¨ ê²€ì¦
        top_level_ok = all(key in evaluation_summary for key in required_top_level)
        print(f"ìƒìœ„ ë ˆë²¨ í‚¤ ì¡´ì¬ í™•ì¸: {'PASS' if top_level_ok else 'FAIL'}")
        
        # ë¬¸ì œ ë ˆë²¨ ê²€ì¦
        problem_level_ok = True
        for problem_result in evaluation_summary['problem_results']:
            if not all(key in problem_result for key in required_problem_level):
                problem_level_ok = False
                break
        print(f"ë¬¸ì œ ë ˆë²¨ í‚¤ ì¡´ì¬ í™•ì¸: {'PASS' if problem_level_ok else 'FAIL'}")
        
        # í…ŒìŠ¤íŠ¸ ë ˆë²¨ ê²€ì¦
        test_level_ok = True
        for problem_result in evaluation_summary['problem_results']:
            for test_result in problem_result['test_results']:
                if not all(key in test_result for key in required_test_level):
                    test_level_ok = False
                    break
            if not test_level_ok:
                break
        print(f"í…ŒìŠ¤íŠ¸ ë ˆë²¨ í‚¤ ì¡´ì¬ í™•ì¸: {'PASS' if test_level_ok else 'FAIL'}")
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        type_checks = [
            (evaluation_summary['overall_accuracy'], (int, float), 'ì „ì²´ ì •í™•ë„'),
            (evaluation_summary['total_correct'], int, 'ì´ ì •ë‹µ ìˆ˜'),
            (evaluation_summary['total_tests'], int, 'ì´ í…ŒìŠ¤íŠ¸ ìˆ˜'),
            (evaluation_summary['problem_results'], list, 'ë¬¸ì œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸')
        ]
        
        type_ok = True
        for value, expected_type, description in type_checks:
            if not isinstance(value, expected_type):
                print(f"{description} íƒ€ì… ì˜¤ë¥˜: {type(value)} (ì˜ˆìƒ: {expected_type})")
                type_ok = False
        
        print(f"ë°ì´í„° íƒ€ì… í™•ì¸: {'PASS' if type_ok else 'FAIL'}")
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸
        try:
            json_str = json.dumps(evaluation_summary, indent=2)
            json_ok = True
        except Exception as e:
            print(f"JSON ì§ë ¬í™” ì˜¤ë¥˜: {e}")
            json_ok = False
        
        print(f"JSON ì§ë ¬í™” ê°€ëŠ¥ì„±: {'PASS' if json_ok else 'FAIL'}")
        
        return top_level_ok and problem_level_ok and test_level_ok and type_ok and json_ok
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª Evaluation ì •í™•ë„ ê³„ì‚° ë¡œì§ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
        
        tests = [
            ("ë‹¨ì¼ ë¬¸ì œ ì •í™•ë„ ê³„ì‚°", self.test_single_problem_accuracy),
            ("ì „ì²´ ì •í™•ë„ ê³„ì‚°", self.test_overall_accuracy_calculation),
            ("ê²½ê³„ ì¼€ì´ìŠ¤", self.test_edge_cases),
            ("ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„", self.test_floating_point_precision),
            ("ê²°ê³¼ ë°ì´í„° êµ¬ì¡°", self.test_result_data_structure)
        ]
        
        results = {}
        for test_name, test_func in tests:
            print(f"{'='*50}")
            print(f"í…ŒìŠ¤íŠ¸: {test_name}")
            print('='*50)
            
            try:
                result = test_func()
                results[test_name] = result
                print(f"{'âœ… PASS' if result else 'âŒ FAIL'}: {test_name}")
            except Exception as e:
                print(f"âŒ í…ŒìŠ¤íŠ¸ '{test_name}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                results[test_name] = False
                import traceback
                traceback.print_exc()
        
        # ê²°ê³¼ ìš”ì•½
        print("\n" + "="*50)
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        
        passed = sum(1 for result in results.values() if result)
        total = len(results)
        
        for test_name, result in results.items():
            status = "âœ… PASS" if result else "âŒ FAIL"
            print(f"{status}: {test_name}")
        
        print(f"\nì „ì²´ ê²°ê³¼: {passed}/{total} í…ŒìŠ¤íŠ¸ í†µê³¼")
        print(f"ì„±ê³µë¥ : {passed/total*100:.1f}%")
        
        return results


def main():
    tester = EvaluationAccuracyTester()
    results = tester.run_all_tests()
    
    # ê²°ê³¼ ì €ì¥
    output_file = "evaluation_accuracy_test_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()