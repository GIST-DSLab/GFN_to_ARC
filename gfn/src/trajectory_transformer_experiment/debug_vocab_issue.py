#!/usr/bin/env python3
"""
Vocabulary ë¬¸ì œ ë””ë²„ê¹…
"""

import torch
import json
import os
import numpy as np
from utils.data_utils import create_vocabulary

def debug_data():
    """ì‹¤ì œ ë°ì´í„°ì˜ í† í° ê°’ ë²”ìœ„ í™•ì¸"""
    print("=== Debugging Vocabulary Issues ===")
    
    # Vocabulary í™•ì¸
    vocab = create_vocabulary()
    print(f"Vocabulary size: {max(vocab.values()) + 1}")
    print(f"Vocabulary: {vocab}")
    
    # ReARC ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    rearc_dir = "../LLM_experiment/data/re-arc/re_arc_extracted/re_arc/tasks"
    
    id_to_hex = {
        86: "25ff71a9",
        139: "6150a2bd",
        149: "67a3c6ac",
        178: "74dd1130",
        240: "9dfd6313"
    }
    
    all_values = set()
    
    for problem_id, hex_id in id_to_hex.items():
        file_path = os.path.join(rearc_dir, f"{hex_id}.json")
        if os.path.exists(file_path):
            print(f"\nChecking problem {problem_id} ({hex_id})")
            
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            # ì²« 3ê°œ ì˜ˆì œë§Œ í™•ì¸
            for i, example in enumerate(data[:3]):
                input_grid = example['input']
                output_grid = example['output']
                
                # Grid ê°’ë“¤ í™•ì¸
                input_values = set(np.array(input_grid).flatten())
                output_values = set(np.array(output_grid).flatten())
                
                all_values.update(input_values)
                all_values.update(output_values)
                
                print(f"  Example {i}: input values {sorted(input_values)}, output values {sorted(output_values)}")
                
                # ìœ„í—˜í•œ ê°’ ì²´í¬ (vocab size ì´ˆê³¼)
                dangerous_input = [v for v in input_values if v >= 22]
                dangerous_output = [v for v in output_values if v >= 22]
                
                if dangerous_input or dangerous_output:
                    print(f"    âš ï¸  DANGEROUS VALUES: input {dangerous_input}, output {dangerous_output}")
    
    print(f"\n=== Summary ===")
    print(f"All unique values found: {sorted(all_values)}")
    print(f"Max value: {max(all_values) if all_values else 'None'}")
    print(f"Values >= vocab_size (22): {[v for v in all_values if v >= 22]}")
    
    return all_values

def test_encoding():
    """ì¸ì½”ë”© í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\n=== Testing Encoding Function ===")
    
    # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ê·¸ë¦¬ë“œ ìƒì„±
    test_grids = [
        [[0, 1, 2], [3, 4, 5], [6, 7, 8]],  # ì •ìƒ ë²”ìœ„
        [[9, 10, 11], [12, 13, 14], [15, 16, 17]],  # vocab ì´ˆê³¼
        [[20, 21, 22], [23, 24, 25], [26, 27, 28]]  # ë§¤ìš° í° ê°’
    ]
    
    for i, grid in enumerate(test_grids):
        print(f"\nTest grid {i+1}: {grid}")
        
        # í˜„ì¬ ì¸ì½”ë”© ë°©ì‹
        flat_grid = np.array(grid).flatten()
        tokens = [int(x) if x < 10 else 10 for x in flat_grid]
        
        print(f"Flattened: {flat_grid.tolist()}")
        print(f"Encoded tokens: {tokens}")
        print(f"Token range: {min(tokens)} - {max(tokens)}")
        
        # ë¬¸ì œ ì²´í¬
        if max(flat_grid) >= 22:
            print(f"  âš ï¸  Original values exceed vocab_size (22)")
        if max(tokens) >= 22:
            print(f"  ğŸš¨ CRITICAL: Encoded tokens exceed vocab_size!")

def fix_encoding():
    """ìˆ˜ì •ëœ ì¸ì½”ë”© í•¨ìˆ˜ ì œì•ˆ"""
    print("\n=== Fixed Encoding Function ===")
    
    def encode_initial_state_fixed(grid_state):
        """ìˆ˜ì •ëœ ì¸ì½”ë”© í•¨ìˆ˜"""
        flat_grid = np.array(grid_state).flatten()
        
        # ëª¨ë“  ê°’ì„ vocab ë²”ìœ„ ë‚´ë¡œ í´ë¨í•‘ (0-9ë§Œ ì‚¬ìš©)
        tokens = [min(int(x), 9) for x in flat_grid]
        
        return tokens
    
    # í…ŒìŠ¤íŠ¸
    test_grid = [[20, 21, 22], [23, 24, 25], [26, 27, 28]]
    print(f"Test grid: {test_grid}")
    
    original_tokens = [int(x) if x < 10 else 10 for x in np.array(test_grid).flatten()]
    fixed_tokens = encode_initial_state_fixed(test_grid)
    
    print(f"Original encoding: {original_tokens}")
    print(f"Fixed encoding: {fixed_tokens}")
    print(f"Max token (original): {max(original_tokens)}")
    print(f"Max token (fixed): {max(fixed_tokens)}")

if __name__ == "__main__":
    all_values = debug_data()
    test_encoding()
    fix_encoding()