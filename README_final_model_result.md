# LLM Experiment Final Model Results

ì´ í´ë”ëŠ” **GFN_to_ARC** í”„ë¡œì íŠ¸ì˜ LLM ì‹¤í—˜ ìµœì¢… ê²°ê³¼ì™€ ì¬í˜„ ê°€ëŠ¥í•œ ëª¨ë¸ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“ í´ë” êµ¬ì¡°

```
final_model_result/
â”œâ”€â”€ README.md                              # ì´ íŒŒì¼
â”œâ”€â”€ REPRODUCTION_TEST.md                   # ëª¨ë¸ ì¬í˜„ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼
â”œâ”€â”€ MODEL_INFO.md                          # ëª¨ë¸ ìƒì„¸ ì •ë³´
â”œâ”€â”€ config_ddp_456.yaml                    # GPU456 ì‹¤í—˜ ì„¤ì • íŒŒì¼
â”œâ”€â”€ utils.py                               # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í”„ë¡¬í”„íŠ¸ ìƒì„±, ì•¡ì…˜ íŒŒì‹± ë“±)
â”œâ”€â”€ original_inference.py                  # ì¶”ë¡  ë° í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ training_unsloth.py                    # Unslothë¥¼ ì‚¬ìš©í•œ ê³ ì† LoRA íŒŒì¸íŠœë‹
â”œâ”€â”€ data_preprocessing.py                  # ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_experiment.py                      # ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ baseline_arc_results.json              # ë² ì´ìŠ¤ë¼ì¸ ARC ê²°ê³¼
â”œâ”€â”€ gpu456_inference_results.json          # GPU456 ì¶”ë¡  ê²°ê³¼
â”œâ”€â”€ gpu456_integrated_prompt_results.json  # í†µí•© í”„ë¡¬í”„íŠ¸ ê²°ê³¼
â”œâ”€â”€ gpu456_reproduction_results.json       # ì¬í˜„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
â”œâ”€â”€ experiment.log                         # ì‹¤í—˜ ë¡œê·¸
â””â”€â”€ training.log                          # í•™ìŠµ ë¡œê·¸
```

## ğŸ¯ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

### ì£¼ìš” ì„±ê³¼
- **ëª¨ë¸**: Llama-3.1-8B-Instruct + LoRA ì–´ëŒ‘í„°
- **ì •í™•ë„**: 25% (5ë¬¸ì œ ì¤‘ 1.25ë¬¸ì œ ì •í™• í•´ê²°)
- **ë°ì´í„°**: ReARC ë°ì´í„°ì…‹ 7ê°œ ë¬¸ì œì—ì„œ í•™ìŠµ/í‰ê°€
- **ë°©ë²•**: Few-shot learning + BARC í˜•ì‹ í”„ë¡¬í”„íŠ¸

### ì„±ëŠ¥ ë¶„ì„
| ë¬¸ì œ ID | ì •í™•ë„ | Pixel ì •í™•ë„ | ìƒíƒœ |
|---------|--------|-------------|------|
| 4258a5f9 | 0.00 | 0.412 | âŒ |
| 445eab21 | 1.00 | 1.000 | âœ… |
| 6f8cd79b | 0.00 | 0.333 | âŒ |
| bb43febb | 0.00 | 0.361 | âŒ |
| bda2d7a6 | 0.25 | 0.543 | ğŸ”¸ |

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •
```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch transformers peft unsloth trl datasets numpy tqdm pyyaml

# GPU ë©”ëª¨ë¦¬ ìµœì†Œ 16GB í•„ìš” (float16 precision)
```

### 2. ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA ì–´ëŒ‘í„° ë¡œë”©
model_path = "/opt/dlami/nvme/seungpil/models_gpu456/unsloth_lora_model"
model = PeftModel.from_pretrained(base_model, model_path)

# í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(model_path)
```

### 3. ì¶”ë¡  ì‹¤í–‰
```python
from utils import create_inference_prompt, parse_action_sequence_from_llm

# í”„ë¡¬í”„íŠ¸ ìƒì„± (few-shot examples í¬í•¨)
prompt = create_inference_prompt(
    input_grid=test_input,
    train_examples=few_shot_examples,
    use_barc_format=True
)

# ëª¨ë¸ ì¶”ë¡ 
inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
outputs = model.generate(
    inputs['input_ids'],
    max_new_tokens=50,
    temperature=0.1,
    do_sample=True
)

# ì•¡ì…˜ ì‹œí€€ìŠ¤ íŒŒì‹±
response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):])
actions = parse_action_sequence_from_llm(response)
```

### 4. ì „ì²´ í‰ê°€ ì‹¤í–‰
```bash
cd final_model_result/
python original_inference.py --config config_ddp_456.yaml --model_path /opt/dlami/nvme/seungpil/models_gpu456/unsloth_lora_model
```

## ğŸ“Š ê²°ê³¼ íŒŒì¼ ì„¤ëª…

### í‰ê°€ ê²°ê³¼
- **`gpu456_inference_results.json`**: ê¸°ë³¸ ì¶”ë¡  ê²°ê³¼
- **`gpu456_integrated_prompt_results.json`**: í†µí•© í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ê²°ê³¼
- **`gpu456_reproduction_results.json`**: ì¬í˜„ í…ŒìŠ¤íŠ¸ ê²°ê³¼

### ë¡œê·¸ íŒŒì¼
- **`experiment.log`**: ì „ì²´ ì‹¤í—˜ ê³¼ì • ë¡œê·¸
- **`training.log`**: ëª¨ë¸ í•™ìŠµ ê³¼ì • ë¡œê·¸

### ì„¤ì • íŒŒì¼
- **`config_ddp_456.yaml`**: GPU456 ì‹¤í—˜ì˜ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •

## ğŸ”¬ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

### ëª¨ë¸ ì•„í‚¤í…ì²˜
- **ë² ì´ìŠ¤ ëª¨ë¸**: meta-llama/Llama-3.1-8B-Instruct
- **íŒŒì¸íŠœë‹**: LoRA (Low-Rank Adaptation)
- **í”„ë¡¬í”„íŠ¸ í˜•ì‹**: BARC (ìƒ‰ìƒ ì½”ë”©) + Few-shot learning

### ì•¡ì…˜ ì‹œí€€ìŠ¤
ëª¨ë¸ì€ ë‹¤ìŒ ì•¡ì…˜ë“¤ì„ í•™ìŠµí•©ë‹ˆë‹¤:
- `left_rotate` (0): 90ë„ ë°˜ì‹œê³„ë°©í–¥ íšŒì „
- `right_rotate` (1): 90ë„ ì‹œê³„ë°©í–¥ íšŒì „
- `horizontal_flip` (2): ìˆ˜í‰ ë’¤ì§‘ê¸°
- `vertical_flip` (3): ìˆ˜ì§ ë’¤ì§‘ê¸°
- `submit` (4): ìµœì¢… ì œì¶œ

### í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
```
Problem: Transform the input grid by applying the correct sequence of transformations.

Examples:
Input:
ğŸŸ¦ğŸŸ¦ğŸŸ¥
ğŸŸ¦ğŸŸ¥ğŸŸ¦
ğŸŸ¥ğŸŸ¦ğŸŸ¦

Actions: [right_rotate,submit]

Now solve:
Input:
ğŸŸ¥ğŸŸ¦ğŸŸ¦
ğŸŸ¦ğŸŸ¦ğŸŸ¥
ğŸŸ¦ğŸŸ¥ğŸŸ¦

Actions:
```

## âœ… ì¬í˜„ ê°€ëŠ¥ì„±

**ì™„ì „ ì¬í˜„ ê°€ëŠ¥**: ì´ í´ë”ì˜ íŒŒì¼ë“¤ë§Œìœ¼ë¡œ GPU456 ëª¨ë¸ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìƒì„¸í•œ ì¬í˜„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ëŠ” `REPRODUCTION_TEST.md`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### í•„ìš”ì‚¬í•­
1. ë² ì´ìŠ¤ ëª¨ë¸: `meta-llama/Llama-3.1-8B-Instruct` (HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ)
2. LoRA ì–´ëŒ‘í„°: `/opt/dlami/nvme/seungpil/models_gpu456/unsloth_lora_model/`
3. GPU ë©”ëª¨ë¦¬: ìµœì†Œ 16GB

## ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥ì„±

### í˜„ì¬ í•œê³„
- ë³µì¡í•œ ë³€í™˜ ì‹œí€€ìŠ¤ì—ì„œ ì–´ë ¤ì›€
- ìƒ‰ìƒ íŒ¨í„´ ì¸ì‹ ë¶€ì •í™•
- í° ê·¸ë¦¬ë“œì—ì„œ ì„±ëŠ¥ ì €í•˜

### ê°œì„  ë°©ì•ˆ
- ë” ë§ì€ í•™ìŠµ ë°ì´í„°
- ë” ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- ë©€í‹°ìŠ¤í… ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- ì•™ìƒë¸” ë°©ë²•

## ğŸ“ ì—°ë½ì²˜

ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ì¬í˜„ì— ì–´ë ¤ì›€ì´ ìˆìœ¼ì‹œë©´ í”„ë¡œì íŠ¸ ê´€ë¦¬ìì—ê²Œ ì—°ë½í•˜ì„¸ìš”.